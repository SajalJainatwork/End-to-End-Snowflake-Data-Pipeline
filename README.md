# ðŸ¦ End-to-End Snowflake Data Pipeline: Czech Banking Project  
**Enterprise-Scale Cloud Data Engineering & Automation**

![Snowflake](https://img.shields.io/badge/Snowflake-Cloud%20Data%20Platform-blue)  
![AWS S3](https://img.shields.io/badge/AWS%20S3-Connected-green)  
![SQL](https://img.shields.io/badge/SQL-Fully%20Automated-orange)  
![Tasks](https://img.shields.io/badge/Tasks-Enabled-informational)

---

## ðŸ“Œ Project Overview

This project simulates a **real-time enterprise data warehouse** using Snowflakeâ€™s cloud-native tools integrated with AWS S3. The data represents operations of a Czech bank â€” covering customer profiles, transactions, loans, and credit cards.

ðŸ”§ Technologies Used:
- **Snowflake** for data warehouse & transformation  
- **AWS S3** for staging/landing zone  
- **Snowpipe** for auto-ingestion from cloud storage  
- **SQL Tasks + Stored Procedures** for ETL automation  

---

## ðŸ§± Architecture & Workflow

### ðŸ”„ Automated Data Flow

```mermaid
graph TD
A[Raw CSVs in AWS S3] --> B(Snowflake External Stage)
B --> C[Snowpipes - Auto Ingest CSVs]
C --> D[Staging Tables in Snowflake]
D --> E[Stored Procedures - Transformations]
E --> F[Automated Scheduled Tasks]
F --> G[Final Reporting Tables]
````

---

## ðŸ—ƒï¸ Database Schema

### âœ… Tables Created

* `DISTRICT`
* `ACCOUNT`
* `ORDER_LIST`
* `LOAN`
* `TRANSACTIONS`
* `CLIENT`
* `DISPOSITION`
* `CARD`

Each table is related via foreign keys, following normalized schema design (see `ERD.pdf` in repo).

---

## ðŸš€ Step-by-Step Setup

### 1ï¸âƒ£ Create Database & Tables

```sql
CREATE DATABASE BANK;
USE BANK;
-- Create tables: DISTRICT, ACCOUNT, ORDER_LIST, LOAN, TRANSACTIONS, CLIENT, DISPOSITION, CARD
```

> SQL scripts available in [`/SQL_SCRIPT/cont_data_load.sql`](SQL_SCRIPT/cont_data_load.sql)

---

### 2ï¸âƒ£ Connect to AWS S3 via External Stage

```sql
CREATE OR REPLACE STORAGE INTEGRATION s3_int
TYPE = EXTERNAL_STAGE
STORAGE_PROVIDER = S3
ENABLED = TRUE
STORAGE_AWS_ROLE_ARN = 'arn:aws:iam::441615131317:role/bankrole'
STORAGE_ALLOWED_LOCATIONS = ('s3://czechbankdata/');

CREATE OR REPLACE STAGE BANK
URL = 's3://czechbankdata'
FILE_FORMAT = (TYPE = 'CSV')
STORAGE_INTEGRATION = s3_int;
```

---

### 3ï¸âƒ£ Snowpipe Configuration for Auto-Ingest

Each folder in S3 is mapped to its respective Snowflake table:

```sql
CREATE OR REPLACE PIPE BANK_SNOWPIPE_ACCOUNT AUTO_INGEST = TRUE AS
COPY INTO BANK.PUBLIC.ACCOUNT
FROM @BANK/Account/
FILE_FORMAT = (TYPE = 'CSV');
```

âœ… Pipes created for:

* `ACCOUNT`, `TRANSACTIONS`, `DISPOSITION`, `DISTRICT`
* `CLIENT`, `ORDER_LIST`, `LOAN`, `CARD`

---

### 4ï¸âƒ£ Load & Validate Data

Manually trigger a pipe (for testing):

```sql
ALTER PIPE BANK_SNOWPIPE_ACCOUNT REFRESH;
```

Validate data counts:

```sql
SELECT COUNT(*) FROM ACCOUNT;
SELECT COUNT(*) FROM TRANSACTIONS;
-- etc.
```

---

### 5ï¸âƒ£ Automate ETL using Stored Procedures + Tasks

Use modular SQL logic to update and transform records.

Sample Stored Procedure:

```sql
CREATE OR REPLACE PROCEDURE UPDATE_BALANCES()
LANGUAGE SQL
AS
$$
BEGIN
  -- Your transformation logic here
END;
$$;
```

Automate execution every 1 minute:

```sql
CREATE OR REPLACE TASK BAL_UPDATE_TASK
WAREHOUSE = COMPUTE_WH
SCHEDULE = '1 MINUTE'
AS CALL UPDATE_BALANCES();

ALTER TASK BAL_UPDATE_TASK RESUME;
```

---

## ðŸ“‚ Repository Structure

```
ðŸ“¦ End-To-End-Data-Analytics-Project_Banking/
â”‚
â”œâ”€â”€ SQL_SCRIPT/                 # All SQL scripts
â”‚   â””â”€â”€ cont_data_load.sql
â”‚
â”œâ”€â”€ Datasets_Raw/              # Original CSVs
â”œâ”€â”€ Datasets_Cleaned/          # Transformed versions
â”œâ”€â”€ Diagram/                   # PNG/PDF of dashboards & ERD
â”œâ”€â”€ Snowflake_continuous_data_loading.docx
â”œâ”€â”€ czec_bank_json_policies.txt
â””â”€â”€ README.md                  # â† This file
```

---

## ðŸ“Š Sample Visual Outputs

* Power BI dashboard snapshot- [ðŸ“„ View Dashboard PDF](https://github.com/AvinashAnalytics/End-to-End-Snowflake-Data-Pipeline/blob/main/DASHBOARD_PBI.pdf)

*  Published analytics visuals--
 ![Published analytics visuals](https://github.com/AvinashAnalytics/End-to-End-Snowflake-Data-Pipeline/blob/main/Diagram/Banking_KPI_PBI.png)

* ODBC connector flow--
  ![ODBC connector flow](https://github.com/AvinashAnalytics/End-to-End-Snowflake-Data-Pipeline/blob/main/Diagram/SNOWFLAKE_PBI_CONNECTION.png)

---

## ðŸ“ˆ Business Value

| Feature              | Benefit                                 |
| -------------------- | --------------------------------------- |
| ðŸ’¡ Auto Ingestion    | Real-time updates from S3               |
| ðŸ”„ Task Automation   | No manual triggers needed               |
| ðŸ§© Normalized Schema | High-quality structure for BI reporting |
| ðŸ“Š Modular SQL       | Maintainable ETL pipeline               |

---

## ðŸ’¬ Analytical Philosophy

> **"Data beats emotions. Every business decision should start with 'What do the numbers say?'"**

